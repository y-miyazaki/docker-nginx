# priority match description
# 1 = 完全一致
# 2   完全一致
# 3 ^~、前方一致
# 4 ~ or ~* 正規表現マッチ
# 5 修飾子なし、前方一致

# added the server block
server {
  listen 80 default_server;
  proxy_request_buffering off;
  # set_real_ip_from 10.0.0.0/8;
  # real_ip_header X-Forwarded-For;
  # real_ip_recursive on;

  location / {
    set $proxy_domain ${PROXY_DOMAIN};
    set $proxy_port ${PROXY_PORT};
    # # when resolver domain ip domain is not fixed, always should update resolver.
    resolver ${PROXY_NAMESERVER} valid=2s;
    proxy_http_version 1.1;
    proxy_ignore_client_abort on;
    # proxy_set_header Host $host;
    proxy_set_header X-Forwarded-Host $host;
    proxy_set_header X-Forwarded-Server $host;
    # for nginx ingress(kubernetes)
    # proxy_set_header X-Forwarded-For $http_x_original_forwarded_for;
    # for GCP
    # proxy_set_header X-Real-IP $remote_addr;
    # proxy_set_header X-Google-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $http_x_forwarded_for;
    proxy_set_header X-Request-Id $request_id;
    proxy_pass ${PROXY_PROTOCOL}://$proxy_domain:$proxy_port;
  }
  # deny .ht file 
  location ~ /\.ht {
    deny all;
  }
  # health check stub page.
  location = /nginx_status {
    stub_status on;
    access_log off;
  }
  # robots page for crawler
  # NOTE
  # If this content has already been indexed by Clowler, it will remain in the search results even if it is later disallowed by robots.txt.
  # In that case, let's control with meta noindex instead of controlling with robots.txt.
  location = /robots.txt {
    access_log off;
    set $robots_txt ${PROXY_ROBOTS_TXT};
    alias /etc/nginx/robots/$robots_txt;
  }
}
