# -----------------------------------------
# error page
# -----------------------------------------
# error_page 400 /400.json;
# error_page 404 /404.json;
# error_page 405 /405.json;
# error_page 415 /415.json;
# error_page 502 /502.json;
# error_page 503 /503.json;
# error_page 504 /504.json;

# added the server block
server {
  listen 80 default_server;
  proxy_request_buffering off;

  location / {
    set $proxy_domain ${PROXY_DOMAIN};
    set $proxy_port ${PROXY_PORT};
    # # when resolver domain ip domain is not fixed, always should update resolver.
    resolver ${PROXY_NAMESERVER} valid=2s;
    proxy_http_version 1.1;
    proxy_ignore_client_abort on;
    # proxy_set_header Host $host;
    proxy_set_header X-Forwarded-Host $host;
    proxy_set_header X-Forwarded-Server $host;
    proxy_set_header X-Forwarded-For $http_x_forwarded_for;
    proxy_set_header X-Request-Id $request_id;
    # app comes from /etc/hosts, Docker added it for us!
    proxy_pass http://$proxy_domain:$proxy_port;
  }
  location ~ \.(css|gif|ico|jpeg|jpg|js|pdf|png|svg|swf|zip|eot|otf|ttf|woff|woff2) {
      expires 1d;
      access_log off;
  }
  # error json files.
  location ~ /[0-9]+.json {
    root /etc/nginx/error;
    allow all;
  }
  # health check stub page.
  location ~ /nginx_status {
    stub_status on;
    access_log off;
  }
  # health check page.
  location ~ /health_?check {
    access_log off;
  }
  # robots page for crawler
  # NOTE
  # If this content has already been indexed by Clowler, it will remain in the search results even if it is later disallowed by robots.txt.
  # In that case, let's control with meta noindex instead of controlling with robots.txt.
  location /robots.txt {
    set $robots_txt ${PROXY_ROBOTS_TXT};
    alias /etc/nginx/robots/$robots_txt;
  }
}
